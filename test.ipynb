{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_pdf import PDFProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = PDFProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_content = r\"D:\\exxonpocaws\\datafiles\\a500.pdf\"\n",
    "s3 = None\n",
    "input_bucket = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_content = chunker.process_pdf(file_content, s3, input_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Lambda Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambda_function import lambda_handler\n",
    "import json\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample context (mock it if needed)\n",
    "class Context:\n",
    "    def __init__(self):\n",
    "        self.aws_request_id = \"test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = Context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the event.json file\n",
    "with open('test_event.json', 'r') as file:\n",
    "    event = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the function\n",
    "response = lambda_handler(event, context)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your local folder for storing the model\n",
    "local_folder = \"./local_models/clip-vit-base-patch32\"\n",
    "\n",
    "# Download and save the model and processor locally\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=local_folder)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=local_folder)\n",
    "\n",
    "# Save the model locally\n",
    "model.save_pretrained(\"./local_models/model/clip-vit-base-patch32\")\n",
    "processor.save_pretrained(\"./local_models/processor/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your local folder for storing the model\n",
    "local_folder = r\"C:\\Users\\prana\\Desktop\\lambda-chunking\\local_models\\paligemma2-3b-pt-224\"\n",
    "# Load model directly\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"google/paligemma2-3b-pt-224\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"google/paligemma2-3b-pt-224\")\n",
    "\n",
    "# Save the model locally\n",
    "model.save_pretrained(local_folder)\n",
    "processor.save_pretrained(local_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your local folder for storing the model\n",
    "local_folder = r\"C:\\Users\\prana\\Desktop\\lambda-chunking\\local_models\\Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "# Save the model locally\n",
    "model.save_pretrained(local_folder)\n",
    "processor.save_pretrained(local_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_folder = \"./local_models/blip-image-captioning-base/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.save_pretrained(local_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "local_folder = \"./local_models/vit-gpt2-image-captioning/\"\n",
    "pipe = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "pipe.save_pretrained(local_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pages = 133\n",
    "num_threads=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = -(-num_pages // num_threads)  # Ceiling division for chunk size\n",
    "page_ranges = [list(range(i, min(i + chunk_size, num_pages))) for i in range(0, num_pages, chunk_size)]\n",
    "\n",
    "print(page_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_ranges = [range(0,34), range(34,68), range(68, 102), range(102, 133)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures=[]\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for pages in page_ranges:\n",
    "        futures.append(executor.submit(pymupdf4llm.to_markdown, r\"C:\\Users\\prana\\Downloads\\archive\\Manuals\\coffee machine1.pdf\", page_chunks=True, pages = pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures[-1].result()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = fitz.open(r\"D:\\exxonpocaws\\datafiles\\a500.pdf\"\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 4):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_count = 133\n",
    "num_threads = 4\n",
    "overlap = 2\n",
    "page_ranges = [\n",
    "    list(range(\n",
    "        max(0, i * (page_count // num_threads) - overlap), \n",
    "        min(page_count, (i + 1) * (page_count // num_threads) + overlap)\n",
    "    ))\n",
    "    for i in range(num_threads)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pymupdf4llm.to_markdown(r\"C:\\Users\\prana\\Downloads\\archive\\Manuals\\exercise bikes.pdf\", page_chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM  \n",
    "from PIL import Image\n",
    "import requests\n",
    "import copy\n",
    "import torch\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = './local_models/Florence-2-base'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype='auto').eval().cuda()\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example(task_prompt, text_input=None):\n",
    "    if text_input is None:\n",
    "        prompt = task_prompt\n",
    "    else:\n",
    "        prompt = task_prompt + text_input\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\n",
    "    generated_ids = model.generate(\n",
    "      input_ids=inputs[\"input_ids\"],\n",
    "      pixel_values=inputs[\"pixel_values\"],\n",
    "      max_new_tokens=1024,\n",
    "      num_beams=3\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "    parsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.width, image.height))\n",
    "\n",
    "    print(parsed_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<DETAILED_CAPTION>': 'This is an animated image. In this image we can see a person wearing goggles. In the back there is a wall. Also there are trees.'}\n"
     ]
    }
   ],
   "source": [
    "task_prompt = '<DETAILED_CAPTION>'\n",
    "context = \"goku just fought someone\"\n",
    "image = Image.open(r\"C:\\Users\\prana\\Desktop\\lambda-chunking\\download.jpg\")\n",
    "run_example(task_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "img_path = \"download.jpg\"\n",
    "instructions = \"Using the following context from the page, summarize the image for RAG using relevant information.\"\n",
    "markdown_context = \"Goku chillin\"\n",
    "\n",
    "try:\n",
    "    # Open the image directly from the file path\n",
    "    image = Image.open(img_path)\n",
    "\n",
    "    # Prepare input for the Qwen model (adjust structure if pipeline requires specific formatting)\n",
    "    full_input = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": f\"{instructions}\\n{markdown_context}\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Generate a summary using the pipeline\n",
    "    result = pipe(text=full_input, images=[image])  # Replace 'pipe' with your actual pipeline object\n",
    "    summary = result[0][\"generated_text\"] if result else \"No summary generated\"\n",
    "except Exception as e:\n",
    "    print(f\"Error generating summary: {e}\")\n",
    "    summary = \"Summary generation failed\"\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layoutmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
